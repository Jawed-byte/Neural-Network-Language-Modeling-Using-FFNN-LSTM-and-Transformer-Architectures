{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":1246668,"datasetId":715814},{"sourceType":"datasetVersion","sourceId":9368120,"datasetId":5681218},{"sourceType":"modelInstanceVersion","sourceId":111541,"isSourceIdPinned":true}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Installation","metadata":{"id":"04oOuEF7zbjp"}},{"cell_type":"code","source":"!pip install torch==2.0.1 torchtext==0.15.2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LrVBQlRozYpP","outputId":"7db397f0-d130-4743-d419-14d6c4134147","execution":{"iopub.status.busy":"2024-09-11T14:41:26.471114Z","iopub.execute_input":"2024-09-11T14:41:26.471513Z","iopub.status.idle":"2024-09-11T14:43:31.779501Z","shell.execute_reply.started":"2024-09-11T14:41:26.471473Z","shell.execute_reply":"2024-09-11T14:43:31.778551Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch==2.0.1\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nCollecting torchtext==0.15.2\n  Downloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.1)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (1.26.4)\nCollecting torchdata==0.6.1 (from torchtext==0.15.2)\n  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (70.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.6.1->torchtext==0.15.2) (1.26.18)\nCollecting cmake (from triton==2.0.0->torch==2.0.1)\n  Downloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting lit (from triton==2.0.0->torch==2.0.1)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\nDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.30.3 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdyfaIC41ZQX","outputId":"ee772ec6-a665-483e-9982-3d75a26c7858","execution":{"iopub.status.busy":"2024-09-11T14:43:40.297006Z","iopub.execute_input":"2024-09-11T14:43:40.297386Z","iopub.status.idle":"2024-09-11T14:43:53.191874Z","shell.execute_reply.started":"2024-09-11T14:43:40.297352Z","shell.execute_reply":"2024-09-11T14:43:53.190928Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import statements","metadata":{}},{"cell_type":"code","source":"import torchtext\nimport string\nimport nltk\nimport re\nimport html\nimport random\nimport subprocess\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport zipfile\nimport os\nimport math\nfrom random import shuffle","metadata":{"id":"1xR2H1XadP9m","execution":{"iopub.status.busy":"2024-09-11T14:44:04.800921Z","iopub.execute_input":"2024-09-11T14:44:04.801772Z","iopub.status.idle":"2024-09-11T14:44:09.397800Z","shell.execute_reply.started":"2024-09-11T14:44:04.801730Z","shell.execute_reply":"2024-09-11T14:44:09.396833Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def split_dataset(file_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n    sentences = []\n    \n    with open(file_path, 'r') as f:\n        para = \"\"\n        for line in tqdm(f, desc=\"Splitting dataset\"):\n            if line.strip():\n                para += line.strip() + \" \"\n            else:\n                if para:\n                    sentences.extend(sent_tokenize(para))\n                    para = \"\"\n        if para:\n            sentences.extend(sent_tokenize(para))\n\n    shuffle(sentences)\n\n    total_sentences = len(sentences)\n    train_size = int(total_sentences * train_ratio)\n    val_size = int(total_sentences * val_ratio)\n    \n    train_sentences = sentences[:train_size]\n    val_sentences = sentences[train_size:train_size + val_size]\n    test_sentences = sentences[train_size + val_size:]\n\n    return train_sentences, val_sentences, test_sentences","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:44:09.399514Z","iopub.execute_input":"2024-09-11T14:44:09.400013Z","iopub.status.idle":"2024-09-11T14:44:09.407656Z","shell.execute_reply.started":"2024-09-11T14:44:09.399978Z","shell.execute_reply":"2024-09-11T14:44:09.406691Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def save_datasets(train_sentences, val_sentences, test_sentences):\n    with open('train.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in train_sentences])\n    with open('dev.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in val_sentences])\n    with open('test.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in test_sentences])","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:44:09.859180Z","iopub.execute_input":"2024-09-11T14:44:09.859782Z","iopub.status.idle":"2024-09-11T14:44:09.866149Z","shell.execute_reply.started":"2024-09-11T14:44:09.859741Z","shell.execute_reply":"2024-09-11T14:44:09.865165Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_sentences, val_sentences, test_sentences = split_dataset('/kaggle/input/auguste-maquet/Auguste_Maquet.txt')\nsave_datasets(train_sentences, val_sentences, test_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:44:12.675412Z","iopub.execute_input":"2024-09-11T14:44:12.676245Z","iopub.status.idle":"2024-09-11T14:44:15.565738Z","shell.execute_reply.started":"2024-09-11T14:44:12.676203Z","shell.execute_reply":"2024-09-11T14:44:15.564723Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Splitting dataset: 128612it [00:02, 46253.79it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_embeddings(emb_file='glove.6B.300d.txt'):\n    unk_emb = torch.zeros(300)\n    embeddings = defaultdict(lambda: unk_emb)\n\n    with open(emb_file, 'r', encoding='ISO-8859-1') as f:\n        for line in tqdm(f, desc=\"Reading embeddings\"):\n            try:\n                split = line.strip().split()\n                word = split[0]\n                vector = torch.tensor([float(x) for x in split[1:]])\n                embeddings[word] = vector\n            except ValueError as e:\n                continue\n\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:44:17.769537Z","iopub.execute_input":"2024-09-11T14:44:17.770585Z","iopub.status.idle":"2024-09-11T14:44:17.778835Z","shell.execute_reply.started":"2024-09-11T14:44:17.770528Z","shell.execute_reply":"2024-09-11T14:44:17.777800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"embeddings = get_embeddings('/kaggle/input/glove/pytorch/default/1/glove.6B.300d.txt')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:47:07.147330Z","iopub.execute_input":"2024-09-11T14:47:07.147705Z","iopub.status.idle":"2024-09-11T14:48:22.255921Z","shell.execute_reply.started":"2024-09-11T14:47:07.147663Z","shell.execute_reply":"2024-09-11T14:48:22.254971Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Reading embeddings: 400000it [01:15, 5326.25it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class TextData(Dataset):\n    def __init__(self, file_path='train.txt', pretrained_emb_dict=embeddings,\n                 frequency_cutoff=1, context_size=5, vocab=None):\n        self.file_path = file_path\n        self.frequency_cutoff = frequency_cutoff\n        self.context_size = context_size\n\n        self.contexts = []\n        self.words = []\n\n        self.frequency_dictionary = defaultdict(lambda: 0)\n        self.vocab = vocab if vocab else []\n\n        self.words2indices = {}\n        self.embeddings = pretrained_emb_dict\n\n        with open(self.file_path, 'r') as f:\n            for line in tqdm(f, desc=\"Obtaining vocabulary and freq counts\"):\n                words = [word.lower() for word in word_tokenize(line)]\n                if not vocab:\n                    self.vocab += words\n                for word in words:\n                    self.frequency_dictionary[word] += 1\n\n            if not vocab:\n                self.vocab = list(set(self.vocab))\n                self.vocab = [word for word in self.vocab if self.frequency_dictionary[word] > self.frequency_cutoff]\n                self.vocab.append('<unk>')\n            self.words2indices = {w: i for i, w in enumerate(self.vocab)}\n\n        embeddings_list = []\n        for word in self.vocab:\n            embeddings_list.append(self.embeddings[word])\n        embeddings_list.append(self.embeddings['<unk>'])\n        self.embeddings = torch.stack(embeddings_list)\n\n        with open(self.file_path, 'r') as f:\n            for line in tqdm(f, desc=\"Creating dataset\"):\n                words = [word.lower() for word in word_tokenize(line)]\n                indices = [self.words2indices[word] if word in self.vocab else (len(self.vocab) - 1)\n                           for word in words]\n                embeddings = [self.embeddings[i] for i in indices]\n\n                for i in range(len(embeddings) - self.context_size):\n                    self.contexts.append(torch.stack(embeddings[i:i + self.context_size]))\n                    self.words.append(indices[i + self.context_size])\n\n        self.contexts = torch.stack(self.contexts)\n        self.words = torch.tensor(self.words)\n\n    def __getitem__(self, idx):\n        return (self.contexts[idx], self.words[idx])\n\n    def __len__(self):\n        return len(self.contexts)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:48:25.115022Z","iopub.execute_input":"2024-09-11T14:48:25.115436Z","iopub.status.idle":"2024-09-11T14:48:25.130268Z","shell.execute_reply.started":"2024-09-11T14:48:25.115389Z","shell.execute_reply":"2024-09-11T14:48:25.129288Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_ds = TextData()\nwith open('vocab.txt', 'w') as f:\n    for word in train_ds.vocab:\n        f.write(word + '\\n')\n\ntest_ds = TextData('test.txt', vocab=train_ds.vocab)\ndev_ds = TextData('dev.txt', vocab=train_ds.vocab)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:48:27.061993Z","iopub.execute_input":"2024-09-11T14:48:27.062402Z","iopub.status.idle":"2024-09-11T14:51:57.487082Z","shell.execute_reply.started":"2024-09-11T14:48:27.062364Z","shell.execute_reply":"2024-09-11T14:51:57.486305Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Obtaining vocabulary and freq counts: 39555it [00:09, 3985.07it/s]\nCreating dataset: 39555it [02:14, 294.45it/s]\nObtaining vocabulary and freq counts: 5652it [00:01, 4217.45it/s]\nCreating dataset: 5652it [00:18, 298.13it/s]\nObtaining vocabulary and freq counts: 11301it [00:02, 4161.19it/s]\nCreating dataset: 11301it [00:37, 297.72it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=300, num_heads=4, num_layers=2, ff_dim=512, dropout=0.1, padding_idx=0):\n        super(TransformerLanguageModel, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n        self.positional_encoding = self._generate_positional_encoding(embedding_dim, max_len=5000)\n\n        self.decoder_layers = nn.TransformerDecoderLayer(\n            d_model=embedding_dim,\n            nhead=num_heads,\n            dim_feedforward=ff_dim,\n            dropout=dropout\n        )\n        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layers, num_layers=num_layers)\n\n        self.fc = nn.Linear(embedding_dim, vocab_size)\n\n    def _generate_positional_encoding(self, embedding_dim, max_len):\n        pos_enc = torch.zeros(max_len, embedding_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n        pos_enc[:, 0::2] = torch.sin(position * div_term)\n        pos_enc[:, 1::2] = torch.cos(position * div_term)\n        return pos_enc.unsqueeze(0)\n\n    def forward(self, batch_ctx):\n        batch_size, seq_len, embedding_dim = batch_ctx.size()\n\n        pos_enc = self.positional_encoding[:, :seq_len, :].to(batch_ctx.device)\n\n        embedded = batch_ctx + pos_enc\n        embedded = embedded.permute(1, 0, 2)\n\n        tgt_mask = self.generate_square_subsequent_mask(seq_len).to(embedded.device)\n\n        transformer_output = self.transformer_decoder(embedded, embedded, tgt_mask=tgt_mask)\n\n        last_hidden_state = transformer_output[-1, :, :]\n        logits = self.fc(last_hidden_state)\n        return logits\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz)) == 1\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def train_epoch(self, dl, optimiser, loss_fn):\n        self.train()\n        for batch in tqdm(dl):\n            optimiser.zero_grad()\n            contexts, words = batch\n            logits = self.forward(contexts)\n            loss = loss_fn(logits, words)\n            loss.backward()\n            optimiser.step()\n\n    def train_model(self, num_epochs, lr=0.1):\n        optimiser = torch.optim.SGD(self.parameters(), lr=lr)\n        loss_fn = nn.CrossEntropyLoss()\n        train_dl = DataLoader(train_ds, batch_size=128)\n        dev_dl = DataLoader(dev_ds, batch_size=128)\n\n        for epoch in range(num_epochs):\n            print(f\"Epoch: {epoch + 1}\")\n            self.train_epoch(train_dl, optimiser, loss_fn)\n            train_loss = self.get_loss(train_dl, loss_fn)\n            print(f\"Loss on train set: {train_loss}\")\n            val_loss = self.get_loss(dev_dl, loss_fn)\n            print(f\"Loss on validation set: {val_loss}\")\n            train_perp = self.get_perp(train_dl, filename='train_perplexity.txt')\n            print(f\"Perplexity on train set: {train_perp}\")\n            val_perp = self.get_perp(dev_dl, filename='test_perplexity.txt')\n            print(f\"Perplexity on validation set: {val_perp}\")\n            print(\"==========================\")\n\n    def get_loss(self, dl, loss_fn):\n        total_loss = 0\n        total_samples = 0\n        self.eval()\n\n        with torch.no_grad():\n            for batch in tqdm(dl):\n                contexts, words = batch\n                pred = self.forward(contexts)\n                loss = loss_fn(pred, words)\n                total_loss += loss.item() * len(words)\n                total_samples += len(words)\n\n        avg_loss = total_loss / total_samples\n        return avg_loss\n\n    def get_perp(self, dl, filename='perplexity_output.txt'):\n        total_loss = 0\n        total_samples = 0\n        loss_fn = nn.CrossEntropyLoss(reduction='sum')\n        self.eval()\n\n        sentence_perplexities = []\n        with open(filename, 'w') as f, torch.no_grad():\n            for batch in tqdm(dl):\n                contexts, words = batch\n\n                pred = self.forward(contexts)\n                loss = loss_fn(pred, words)\n                perplexity = torch.exp(loss / len(words))\n\n                sentence = ' '.join([train_ds.vocab[idx] for idx in words.tolist()])\n                f.write(f\"{sentence}\\t{perplexity.item()}\\n\")\n                \n                total_loss += loss.item()\n                total_samples += len(words)\n\n                sentence_perplexities.append(perplexity.item())\n\n            avg_loss = total_loss / total_samples\n            avg_perplexity = math.exp(avg_loss)\n\n            f.write(f\"Average perplexity: {avg_perplexity}\\n\")\n            print(f\"Average perplexity: {avg_perplexity}\")\n\n        return avg_perplexity","metadata":{"execution":{"iopub.status.busy":"2024-09-11T16:08:03.234531Z","iopub.execute_input":"2024-09-11T16:08:03.234919Z","iopub.status.idle":"2024-09-11T16:08:03.259481Z","shell.execute_reply.started":"2024-09-11T16:08:03.234880Z","shell.execute_reply":"2024-09-11T16:08:03.258499Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"transformer_lm = TransformerLanguageModel(len(train_ds.vocab))\ntransformer_lm.train_model(num_epochs=10)\n\ntorch.save(transformer_lm, '10epochs_transformer.pth')\n\ntest_dl = DataLoader(test_ds, batch_size=128)\nperp = transformer_lm.get_perp(test_dl)\nprint(perp)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T00:25:58.266647Z","iopub.execute_input":"2024-09-11T00:25:58.267335Z","iopub.status.idle":"2024-09-11T02:47:24.506865Z","shell.execute_reply.started":"2024-09-11T00:25:58.267294Z","shell.execute_reply":"2024-09-11T02:47:24.505694Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:14<00:00, 11.55it/s]\n100%|██████████| 5013/5013 [02:41<00:00, 31.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.906577545409246\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:45<00:00, 31.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.913811779087805\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:47<00:00, 29.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 135.17598815194546\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:48<00:00, 29.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 136.1574285422968\n==========================\nEpoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:49<00:00, 10.67it/s]\n100%|██████████| 5013/5013 [02:42<00:00, 30.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.671896824147495\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:45<00:00, 31.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.722732277158658\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:42<00:00, 30.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 106.90032135433643\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:46<00:00, 30.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 112.47514649106964\n==========================\nEpoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:42<00:00, 10.85it/s]\n100%|██████████| 5013/5013 [02:35<00:00, 32.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.51844811830664\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:44<00:00, 32.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.606743531652158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:34<00:00, 32.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 91.69319052319733\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:43<00:00, 32.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 100.15745840217254\n==========================\nEpoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:32<00:00, 11.07it/s]\n100%|██████████| 5013/5013 [02:38<00:00, 31.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.4310084936483145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:40<00:00, 35.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.555966797985258\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:25<00:00, 34.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 84.01610390708842\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:43<00:00, 32.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 95.19874871731507\n==========================\nEpoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:24<00:00, 11.27it/s]\n100%|██████████| 5013/5013 [02:34<00:00, 32.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.347354815055359\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:43<00:00, 32.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.510279294564244\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:30<00:00, 33.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 77.27378888128897\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:42<00:00, 33.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 90.94721602673972\n==========================\nEpoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:19<00:00, 11.40it/s]\n100%|██████████| 5013/5013 [02:29<00:00, 33.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.287467683179539\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:42<00:00, 33.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.4882939310811825\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:29<00:00, 33.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 72.78192804019207\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:42<00:00, 33.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 88.96952817772743\n==========================\nEpoch: 7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:24<00:00, 11.29it/s]\n100%|██████████| 5013/5013 [02:27<00:00, 34.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.209724129290699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:41<00:00, 34.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.450416825831654\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:31<00:00, 33.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 67.33796067654029\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:41<00:00, 34.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 85.66264296395414\n==========================\nEpoch: 8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:17<00:00, 11.45it/s]\n100%|██████████| 5013/5013 [02:34<00:00, 32.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.164285963914807\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:43<00:00, 32.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.4423787102413685\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:28<00:00, 33.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 64.34672015589415\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:42<00:00, 33.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 84.9768367255146\n==========================\nEpoch: 9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:18<00:00, 11.44it/s]\n100%|██████████| 5013/5013 [02:27<00:00, 34.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.119091849666926\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:43<00:00, 33.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.439525863883796\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:31<00:00, 33.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 61.503362595472794\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:44<00:00, 32.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 84.73475633964333\n==========================\nEpoch: 10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [07:42<00:00, 10.85it/s]\n100%|██████████| 5013/5013 [02:42<00:00, 30.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.087679990099404\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:46<00:00, 30.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.44571960182354\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5013/5013 [02:40<00:00, 31.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 59.601455213039934\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1432/1432 [00:45<00:00, 31.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 85.26120988946136\n==========================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 719/719 [00:23<00:00, 30.87it/s]","output_type":"stream"},{"name":"stdout","text":"83.03964072816288\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = TransformerLanguageModel(vocab_size=len(train_ds.vocab))\n\ntrain_dl = DataLoader(train_ds, batch_size=128)\ndev_dl = DataLoader(dev_ds, batch_size=128)\ntest_dl = DataLoader(test_ds, batch_size=128)\n\ntrain_perplexities, val_perplexities = model.train_model(\n    num_epochs=10, \n    lr=0.1, \n    train_dl=train_dl, \n    dev_dl=dev_dl\n)\n\nmodel.get_perp(train_dl, filename='train_perplexity.txt')\nmodel.get_perp(test_dl, filename='test_perplexity.txt')\nprint(\"Perplexity files generated for both train and test sets.\")","metadata":{},"execution_count":null,"outputs":[]}]}