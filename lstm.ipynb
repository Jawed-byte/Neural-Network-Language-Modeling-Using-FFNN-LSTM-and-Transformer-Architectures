{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceId":9366457,"sourceType":"datasetVersion","datasetId":5679939},{"sourceId":111540,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":93469,"modelId":117681}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Installation","metadata":{"id":"04oOuEF7zbjp"}},{"cell_type":"code","source":"!pip install torch==2.0.1 torchtext==0.15.2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LrVBQlRozYpP","outputId":"7db397f0-d130-4743-d419-14d6c4134147","execution":{"iopub.status.busy":"2024-09-11T10:27:49.950454Z","iopub.execute_input":"2024-09-11T10:27:49.950757Z","iopub.status.idle":"2024-09-11T10:29:49.704426Z","shell.execute_reply.started":"2024-09-11T10:27:49.950723Z","shell.execute_reply":"2024-09-11T10:29:49.703490Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch==2.0.1\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nCollecting torchtext==0.15.2\n  Downloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.1)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.15.2) (1.26.4)\nCollecting torchdata==0.6.1 (from torchtext==0.15.2)\n  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (70.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.6.1->torchtext==0.15.2) (1.26.18)\nCollecting cmake (from triton==2.0.0->torch==2.0.1)\n  Downloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting lit (from triton==2.0.0->torch==2.0.1)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.15.2) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\nDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m502.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cmake-3.30.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m776.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.30.3 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdyfaIC41ZQX","outputId":"ee772ec6-a665-483e-9982-3d75a26c7858","execution":{"iopub.status.busy":"2024-09-11T10:29:49.706273Z","iopub.execute_input":"2024-09-11T10:29:49.706626Z","iopub.status.idle":"2024-09-11T10:30:02.949268Z","shell.execute_reply.started":"2024-09-11T10:29:49.706588Z","shell.execute_reply":"2024-09-11T10:30:02.948301Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import statements","metadata":{}},{"cell_type":"code","source":"import torchtext\nimport string\nimport nltk\nimport re\nimport html\nimport random\nimport subprocess\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport zipfile\nimport os\nimport math\nfrom random import shuffle","metadata":{"id":"1xR2H1XadP9m","execution":{"iopub.status.busy":"2024-09-11T10:30:02.950704Z","iopub.execute_input":"2024-09-11T10:30:02.951043Z","iopub.status.idle":"2024-09-11T10:30:07.120091Z","shell.execute_reply.started":"2024-09-11T10:30:02.951004Z","shell.execute_reply":"2024-09-11T10:30:07.119340Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def split_dataset(file_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n    sentences = []\n    \n    with open(file_path, 'r') as f:\n        para = \"\"\n        for line in tqdm(f, desc=\"Splitting dataset\"):\n            if line.strip():\n                para += line.strip() + \" \"\n            else:\n                if para:\n                    sentences.extend(sent_tokenize(para))\n                    para = \"\"\n        if para:\n            sentences.extend(sent_tokenize(para))\n\n    shuffle(sentences)\n\n    total_sentences = len(sentences)\n    train_size = int(total_sentences * train_ratio)\n    val_size = int(total_sentences * val_ratio)\n    \n    train_sentences = sentences[:train_size]\n    val_sentences = sentences[train_size:train_size + val_size]\n    test_sentences = sentences[train_size + val_size:]\n\n    return train_sentences, val_sentences, test_sentences","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:30:07.122744Z","iopub.execute_input":"2024-09-11T10:30:07.123672Z","iopub.status.idle":"2024-09-11T10:30:07.131120Z","shell.execute_reply.started":"2024-09-11T10:30:07.123623Z","shell.execute_reply":"2024-09-11T10:30:07.130147Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def save_datasets(train_sentences, val_sentences, test_sentences):\n    with open('train.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in train_sentences])\n    with open('dev.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in val_sentences])\n    with open('test.txt', 'w') as f:\n        f.writelines([s + '\\n' for s in test_sentences])","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:30:07.132826Z","iopub.execute_input":"2024-09-11T10:30:07.133231Z","iopub.status.idle":"2024-09-11T10:30:07.147470Z","shell.execute_reply.started":"2024-09-11T10:30:07.133182Z","shell.execute_reply":"2024-09-11T10:30:07.146625Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_sentences, val_sentences, test_sentences = split_dataset('/kaggle/input/dataset/Auguste_Maquet.txt')\nsave_datasets(train_sentences, val_sentences, test_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:30:07.148534Z","iopub.execute_input":"2024-09-11T10:30:07.148822Z","iopub.status.idle":"2024-09-11T10:30:10.035032Z","shell.execute_reply.started":"2024-09-11T10:30:07.148791Z","shell.execute_reply":"2024-09-11T10:30:10.034112Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Splitting dataset: 128612it [00:02, 46237.92it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_embeddings(emb_file='glove.6B.300d.txt'):\n    unk_emb = torch.zeros(300)  # Placeholder embedding for unknown words\n    embeddings = defaultdict(lambda: unk_emb)\n\n    with open(emb_file, 'r', encoding='ISO-8859-1') as f:\n        for line in tqdm(f, desc=\"Reading embeddings\"):\n            try:\n                split = line.strip().split()\n                word = split[0]\n                vector = torch.tensor([float(x) for x in split[1:]])  # Corrected line\n                embeddings[word] = vector\n            except ValueError as e:\n                continue\n\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-09-11T10:30:10.036275Z","iopub.execute_input":"2024-09-11T10:30:10.036678Z","iopub.status.idle":"2024-09-11T10:30:10.043515Z","shell.execute_reply.started":"2024-09-11T10:30:10.036633Z","shell.execute_reply":"2024-09-11T10:30:10.042663Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"embeddings = get_embeddings('/kaggle/input/glove/pytorch/default/1/glove.6B.300d.txt')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T11:12:42.935374Z","iopub.execute_input":"2024-09-11T11:12:42.935756Z","iopub.status.idle":"2024-09-11T11:13:56.567255Z","shell.execute_reply.started":"2024-09-11T11:12:42.935719Z","shell.execute_reply":"2024-09-11T11:13:56.566272Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Reading embeddings: 400000it [01:13, 5433.58it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class TextData(Dataset):\n    def __init__(self, file_path='train.txt', pretrained_emb_dict=embeddings,\n                 frequency_cutoff=1, context_size=5, vocab=None):\n        self.file_path = file_path\n        self.frequency_cutoff = frequency_cutoff\n        self.context_size = context_size\n\n        self.contexts = []\n        self.words = []\n\n        self.frequency_dictionary = defaultdict(lambda: 0)\n        self.vocab = vocab if vocab else []\n\n        self.words2indices = {}\n        self.embeddings = pretrained_emb_dict\n\n        with open(self.file_path, 'r') as f:\n            for line in tqdm(f, desc=\"Obtaining vocabulary and freq counts\"):\n                words = [word.lower() for word in word_tokenize(line)]\n                if not vocab:\n                    self.vocab += words\n                for word in words:\n                    self.frequency_dictionary[word] += 1\n\n            if not vocab:\n                self.vocab = list(set(self.vocab))\n                self.vocab = [word for word in self.vocab if self.frequency_dictionary[word] > self.frequency_cutoff]\n                self.vocab.append('<unk>')\n            self.words2indices = {w: i for i, w in enumerate(self.vocab)}\n\n        embeddings_list = []\n        for word in self.vocab:\n            embeddings_list.append(self.embeddings[word])\n        embeddings_list.append(self.embeddings['<unk>'])\n        self.embeddings = torch.stack(embeddings_list)\n\n        with open(self.file_path, 'r') as f:\n            for line in tqdm(f, desc=\"Creating dataset\"):\n                words = [word.lower() for word in word_tokenize(line)]\n                indices = [self.words2indices[word] if word in self.vocab else (len(self.vocab) - 1)\n                           for word in words]\n                embeddings = [self.embeddings[i] for i in indices]\n\n                for i in range(len(embeddings) - self.context_size):\n                    self.contexts.append(torch.stack(embeddings[i:i + self.context_size]))\n                    self.words.append(indices[i + self.context_size])\n\n        self.contexts = torch.stack(self.contexts)\n        self.words = torch.tensor(self.words)\n\n    def __getitem__(self, idx):\n        return (self.contexts[idx], self.words[idx])\n\n    def __len__(self):\n        return len(self.contexts)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T11:14:10.214733Z","iopub.execute_input":"2024-09-11T11:14:10.215478Z","iopub.status.idle":"2024-09-11T11:14:10.233050Z","shell.execute_reply.started":"2024-09-11T11:14:10.215434Z","shell.execute_reply":"2024-09-11T11:14:10.232151Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_ds = TextData()\nwith open('vocab.txt', 'w') as f:\n    for word in train_ds.vocab:\n        f.write(word + '\\n')\n\ntest_ds = TextData('test.txt', vocab=train_ds.vocab)\ndev_ds = TextData('dev.txt', vocab=train_ds.vocab)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T11:14:27.889849Z","iopub.execute_input":"2024-09-11T11:14:27.890227Z","iopub.status.idle":"2024-09-11T11:17:43.941663Z","shell.execute_reply.started":"2024-09-11T11:14:27.890188Z","shell.execute_reply":"2024-09-11T11:17:43.940788Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Obtaining vocabulary and freq counts: 39555it [00:09, 4059.13it/s]\nCreating dataset: 39555it [02:02, 322.10it/s]\nObtaining vocabulary and freq counts: 5652it [00:01, 4002.83it/s]\nCreating dataset: 5652it [00:17, 321.76it/s]\nObtaining vocabulary and freq counts: 11301it [00:02, 3972.54it/s]\nCreating dataset: 11301it [00:36, 312.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class LSTM_LanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=300, num_layers=2, padding_idx=0):\n        super(LSTM_LanguageModel, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.padding_idx = padding_idx\n        \n        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, batch_ctx):\n        batch_size = batch_ctx.size(0)\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(batch_ctx.device)  # Hidden state\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(batch_ctx.device)  # Cell state\n\n        lstm_out, (hn, cn) = self.lstm(batch_ctx, (h0, c0))  # lstm_out shape: (batch_size, context_size, hidden_dim)\n\n        last_hidden_state = lstm_out[:, -1, :]  # Last time step\n        logits = self.fc(last_hidden_state)\n\n        return logits\n\n    def train_epoch(self, dl, optimiser, loss_fn):\n        super().train()  # Ensure model is in training mode\n        for batch in tqdm(dl):\n            optimiser.zero_grad()\n            contexts, words = batch\n\n            # Forward pass\n            logits = self.forward(contexts)\n\n            # Compute loss, ignoring padding tokens\n            loss = loss_fn(logits, words)\n            loss.backward()\n\n            optimiser.step()\n\n    def train(self, num_epochs, lr=0.1):\n        optimiser = torch.optim.SGD(self.parameters(), lr=lr)\n        # Use ignore_index to skip padding tokens in the loss computation\n        loss_fn = nn.CrossEntropyLoss(ignore_index=self.padding_idx)\n        train_dl = DataLoader(train_ds, batch_size=128)\n        dev_dl = DataLoader(dev_ds, batch_size=128)\n\n        for epoch in range(num_epochs):\n            print(\"Epoch:\", epoch + 1)\n            self.train_epoch(train_dl, optimiser, loss_fn)\n            train_loss = self.get_loss(train_dl, loss_fn)\n            print(\"Loss on train set:\", train_loss)\n            val_loss = self.get_loss(dev_dl, loss_fn)\n            print(\"Loss on validation set:\", val_loss)\n            train_perp = self.get_perp(train_dl)\n            print(\"Perplexity on train set:\", train_perp)\n            val_perp = self.get_perp(dev_dl)\n            print(\"Perplexity on validation set:\", val_perp)\n\n        return train_perp, val_perp\n\n    def get_loss(self, dl, loss_fn):\n        total_loss = 0\n        total_samples = 0\n\n        for batch in tqdm(dl):\n            contexts, words = batch\n            pred = self.forward(contexts)\n            loss = loss_fn(pred, words)\n            total_loss += loss.item() * len(words)\n            total_samples += (words != self.padding_idx).sum().item()  # Exclude padding tokens\n\n        avg_loss = total_loss / total_samples\n        return avg_loss\n\n    def get_perp(self, dl, filename='perplexity_output.txt'):\n        total_loss = 0\n        total_samples = 0\n        loss_fn = nn.CrossEntropyLoss(reduction='sum')\n        self.eval()\n\n        sentence_perplexities = []\n        with open(filename, 'w') as f, torch.no_grad():\n            for batch in tqdm(dl):\n                contexts, words = batch\n\n                # Forward pass\n                pred = self.forward(contexts)\n\n                # Calculate loss for the batch\n                loss = loss_fn(pred, words)\n                \n                # Calculate perplexity for the batch\n                perplexity = torch.exp(loss / len(words))\n\n                # Format the contexts (as a sentence) and their corresponding perplexity\n                sentence = ' '.join([train_ds.vocab[idx] for idx in words.tolist()])\n                f.write(f\"{sentence}\\t{perplexity.item()}\\n\")\n                \n                total_loss += loss.item()\n                total_samples += len(words)\n\n                sentence_perplexities.append(perplexity.item())\n\n            avg_loss = total_loss / total_samples\n            avg_perplexity = math.exp(avg_loss)\n\n            f.write(f\"Average perplexity: {avg_perplexity}\\n\")\n            print(f\"Average perplexity: {avg_perplexity}\")\n\n        return avg_perplexity\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T13:16:03.271116Z","iopub.execute_input":"2024-09-11T13:16:03.271527Z","iopub.status.idle":"2024-09-11T13:16:03.290632Z","shell.execute_reply.started":"2024-09-11T13:16:03.271489Z","shell.execute_reply":"2024-09-11T13:16:03.289737Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"lstm_lm = LSTM_LanguageModel(len(train_ds.vocab))\nlstm_lm.train(10)\n\ntorch.save(lstm_lm, '10epochs_lstm.pth')\n\ntest_dl = DataLoader(test_ds, batch_size=128)\nperp = lstm_lm.get_perp(test_dl)\nprint(perp)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:08:24.677806Z","iopub.execute_input":"2024-09-10T18:08:24.678114Z","iopub.status.idle":"2024-09-10T19:39:22.743204Z","shell.execute_reply.started":"2024-09-10T18:08:24.678066Z","shell.execute_reply":"2024-09-10T19:39:22.742395Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:46<00:00, 17.48it/s]\n100%|██████████| 5012/5012 [01:37<00:00, 51.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.747939432516416\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 51.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.69088221584073\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:38<00:00, 51.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 313.54391578664263\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 51.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 296.1547778613643\n==========================\nEpoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:51<00:00, 17.19it/s]\n100%|██████████| 5012/5012 [01:37<00:00, 51.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.328265357257279\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.276204497284666\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:38<00:00, 50.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 206.0801884275392\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 195.6259656054754\n==========================\nEpoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:46<00:00, 17.49it/s]\n100%|██████████| 5012/5012 [01:38<00:00, 50.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.113164464993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.068552115595233\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:39<00:00, 50.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 166.19544328587557\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:29<00:00, 49.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 158.94402808068557\n==========================\nEpoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:47<00:00, 17.42it/s]\n100%|██████████| 5012/5012 [01:38<00:00, 50.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.982197419356609\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.945078656192627\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:38<00:00, 50.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 145.79440135571838\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 140.48190018829288\n==========================\nEpoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:47<00:00, 17.42it/s]\n100%|██████████| 5012/5012 [01:39<00:00, 50.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.885199653671509\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.8569092500773685\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:40<00:00, 49.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 132.3168802482339\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 128.62603619728955\n==========================\nEpoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:50<00:00, 17.28it/s]\n100%|██████████| 5012/5012 [01:38<00:00, 50.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.80463574523882\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.785755237756645\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:40<00:00, 49.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 122.07501651566169\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:29<00:00, 49.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 119.79180021925261\n==========================\nEpoch: 7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:54<00:00, 17.00it/s]\n100%|██████████| 5012/5012 [01:40<00:00, 50.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.737497588246888\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.728671559450408\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:40<00:00, 49.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 114.14819818741476\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:30<00:00, 48.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 113.14515585470903\n==========================\nEpoch: 8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:51<00:00, 17.21it/s]\n100%|██████████| 5012/5012 [01:39<00:00, 50.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.680494013938674\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.682366941078469\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:38<00:00, 50.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 107.82332564221923\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 108.02546010759241\n==========================\nEpoch: 9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:46<00:00, 17.49it/s]\n100%|██████████| 5012/5012 [01:38<00:00, 50.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.631396518729677\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.644315670896575\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:39<00:00, 50.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 102.65732693240271\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 50.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 103.99217655681672\n==========================\nEpoch: 10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [04:45<00:00, 17.54it/s]\n100%|██████████| 5012/5012 [01:37<00:00, 51.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 4.588082728900541\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:28<00:00, 51.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 4.612161095689533\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5012/5012 [01:37<00:00, 51.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 98.30577055114044\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1452/1452 [00:27<00:00, 52.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 100.70154031545744\n==========================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 699/699 [00:13<00:00, 50.76it/s]","output_type":"stream"},{"name":"stdout","text":"99.41911908367919\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dl = DataLoader(test_ds, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T11:19:49.090290Z","iopub.execute_input":"2024-09-11T11:19:49.091130Z","iopub.status.idle":"2024-09-11T11:19:49.095480Z","shell.execute_reply.started":"2024-09-11T11:19:49.091091Z","shell.execute_reply":"2024-09-11T11:19:49.094620Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def experiment(train_ds, dev_ds, test_ds, vocab_size):\n    train_dl = DataLoader(train_ds, batch_size=128)\n    dev_dl = DataLoader(dev_ds, batch_size=128)\n    test_dl = DataLoader(test_ds, batch_size=128)\n\n    hyperparams = [\n        {'lr': 0.01, 'hidden_dim': 300, 'num_layers': 2, 'optim': 'SGD'},\n        {'lr': 0.001, 'hidden_dim': 400, 'num_layers': 2, 'optim': 'Adam'},\n        {'lr': 0.01, 'hidden_dim': 500, 'num_layers': 3, 'optim': 'SGD'}\n    ]\n\n    train_perplexities, dev_perplexities, test_perplexities = [], [], []\n\n    for params in hyperparams:\n        model = LSTM_LanguageModel(vocab_size, hidden_dim=params['hidden_dim'],\n                                   num_layers=params['num_layers'])\n        \n        print(f\"Training with hyperparams: {params}\")\n        train_perp, val_perp = model.train(num_epochs=10, lr=params['lr'])\n        \n        train_perplexities.append(train_perp)\n        dev_perplexities.append(val_perp)\n        \n        test_perp = model.get_perp(test_dl)\n        test_perplexities.append(test_perp)\n        print(f\"Test perplexity: {test_perp:.4f}\")\n        print(\"==========================\")\n\n    return hyperparams, train_perplexities, dev_perplexities, test_perplexities\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:01:21.287632Z","iopub.execute_input":"2024-09-11T14:01:21.288009Z","iopub.status.idle":"2024-09-11T14:01:21.298674Z","shell.execute_reply.started":"2024-09-11T14:01:21.287975Z","shell.execute_reply":"2024-09-11T14:01:21.297686Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(train_ds.vocab)\nhyperparams, train_perplexities, dev_perplexities, test_perplexities = experiment(train_ds, dev_ds, test_ds, vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-09-11T14:01:24.172023Z","iopub.execute_input":"2024-09-11T14:01:24.172848Z","iopub.status.idle":"2024-09-11T21:19:34.826728Z","shell.execute_reply.started":"2024-09-11T14:01:24.172808Z","shell.execute_reply":"2024-09-11T21:19:34.825823Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Training with hyperparams: {'lr': 0.01, 'hidden_dim': 300, 'num_layers': 2, 'optim': 'SGD'}\nEpoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:37<00:00, 18.02it/s]\n100%|██████████| 5009/5009 [01:32<00:00, 54.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.3540164599362505\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:26<00:00, 53.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.306828181843887\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:36<00:00, 51.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 574.7967269437164\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:26<00:00, 54.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 548.3030701781943\n==========================\nEpoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:35<00:00, 18.21it/s]\n100%|██████████| 5009/5009 [01:33<00:00, 53.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.201055275868876\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:26<00:00, 53.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.152244640936316\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:34<00:00, 53.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 493.26930172669944\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 53.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 469.77067069620705\n==========================\nEpoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:35<00:00, 18.17it/s]\n100%|██████████| 5009/5009 [01:33<00:00, 53.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.157831512032196\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:26<00:00, 53.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.108448728524875\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:30<00:00, 55.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 472.40256405668475\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:26<00:00, 54.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 449.6406591418234\n==========================\nEpoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:32<00:00, 18.40it/s]\n100%|██████████| 5009/5009 [01:29<00:00, 55.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.12933931142993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:25<00:00, 55.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.0792917560087405\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:29<00:00, 55.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 459.1327167119147\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:25<00:00, 55.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 436.7197810003642\n==========================\nEpoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:24<00:00, 18.92it/s]\n100%|██████████| 5009/5009 [01:32<00:00, 54.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.087814528303627\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 52.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.037240589737055\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:35<00:00, 52.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 440.4577505557706\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 51.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 418.735974872696\n==========================\nEpoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:36<00:00, 18.12it/s]\n100%|██████████| 5009/5009 [01:35<00:00, 52.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.014528247838417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 52.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.963981943245675\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:35<00:00, 52.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 409.3326897797428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 52.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 389.15664278503954\n==========================\nEpoch: 7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:43<00:00, 17.64it/s]\n100%|██████████| 5009/5009 [01:37<00:00, 51.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.93778224093169\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 51.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.886528737381191\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:38<00:00, 50.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 379.0932590589364\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 52.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 360.1529266072926\n==========================\nEpoch: 8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:46<00:00, 17.49it/s]\n100%|██████████| 5009/5009 [01:36<00:00, 51.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.854466985510365\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 51.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.802483651614813\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:38<00:00, 50.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 348.7889409395917\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 49.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 331.1209285195081\n==========================\nEpoch: 9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:47<00:00, 17.44it/s]\n100%|██████████| 5009/5009 [01:36<00:00, 52.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.7694255164288375\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:27<00:00, 51.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.717125480677383\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:38<00:00, 50.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 320.35364186963955\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 51.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 304.0297263517197\n==========================\nEpoch: 10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:49<00:00, 17.28it/s]\n100%|██████████| 5009/5009 [01:39<00:00, 50.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 5.7050656540903555\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 49.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 5.652790435707794\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [01:39<00:00, 50.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 300.38520091834016\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:28<00:00, 50.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 285.08587073401924\n==========================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 717/717 [00:13<00:00, 53.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test perplexity: 289.0636\n==========================\nTraining with hyperparams: {'lr': 0.001, 'hidden_dim': 400, 'num_layers': 2, 'optim': 'Adam'}\nEpoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:37<00:00, 12.59it/s]\n100%|██████████| 5009/5009 [02:16<00:00, 36.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 9.319851136009891\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 38.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 9.31948883237611\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:13<00:00, 37.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 11157.32080648237\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 36.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 11153.27920079933\n==========================\nEpoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:21<00:00, 13.14it/s]\n100%|██████████| 5009/5009 [02:14<00:00, 37.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 9.039482193311608\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 37.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 9.03722899414485\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:12<00:00, 37.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 8429.411120290535\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:36<00:00, 39.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 8410.439359782013\n==========================\nEpoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:19<00:00, 13.20it/s]\n100%|██████████| 5009/5009 [02:15<00:00, 37.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 7.761687332494042\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:39<00:00, 36.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 7.745292163062528\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:16<00:00, 36.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 2348.8645789684147\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 2310.6685172691136\n==========================\nEpoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:35<00:00, 12.66it/s]\n100%|██████████| 5009/5009 [02:16<00:00, 36.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 7.039148729672666\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:39<00:00, 36.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.996763251996599\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:18<00:00, 36.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 1140.4163906697722\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:40<00:00, 35.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 1093.0893715089544\n==========================\nEpoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:27<00:00, 12.92it/s]\n100%|██████████| 5009/5009 [02:15<00:00, 37.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.712701361051769\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.665415054755244\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:14<00:00, 37.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 822.7902937102484\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 37.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 784.7891278517193\n==========================\nEpoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:24<00:00, 13.04it/s]\n100%|██████████| 5009/5009 [02:12<00:00, 37.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.56958583691224\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.522708934409646\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:14<00:00, 37.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 713.0744528500194\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 680.4191017414829\n==========================\nEpoch: 7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:25<00:00, 12.98it/s]\n100%|██████████| 5009/5009 [02:14<00:00, 37.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.479155372473592\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.4321083494501545\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:16<00:00, 36.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 651.4205061546937\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 621.4828707634249\n==========================\nEpoch: 8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:36<00:00, 12.65it/s]\n100%|██████████| 5009/5009 [02:16<00:00, 36.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.4132475092227175\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 37.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.36604584621064\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:13<00:00, 37.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 609.87103038943\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:38<00:00, 37.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 581.7529343708285\n==========================\nEpoch: 9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:31<00:00, 12.79it/s]\n100%|██████████| 5009/5009 [02:15<00:00, 36.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.363260813948391\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 38.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.3158497136498255\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:12<00:00, 37.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 580.1349877425141\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 37.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 553.2719836698568\n==========================\nEpoch: 10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [06:23<00:00, 13.07it/s]\n100%|██████████| 5009/5009 [02:09<00:00, 38.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.325324059687017\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:36<00:00, 39.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.277608202430008\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [02:08<00:00, 39.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 558.5387842443762\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [00:37<00:00, 38.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 532.5134750012134\n==========================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 717/717 [00:18<00:00, 39.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test perplexity: 537.2356\n==========================\nTraining with hyperparams: {'lr': 0.01, 'hidden_dim': 500, 'num_layers': 3, 'optim': 'SGD'}\nEpoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:10<00:00,  6.86it/s]\n100%|██████████| 5009/5009 [03:59<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.3593084640194\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 20.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.3123027113068835\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [03:58<00:00, 20.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 577.8466164658313\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 21.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 551.3130029559511\n==========================\nEpoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:33<00:00,  6.65it/s]\n100%|██████████| 5009/5009 [04:07<00:00, 20.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.19059168739768\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.14129875589718\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:04<00:00, 20.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 488.1348440203312\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:11<00:00, 20.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 464.656654715623\n==========================\nEpoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:21<00:00,  6.75it/s]\n100%|██████████| 5009/5009 [04:01<00:00, 20.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.151679601956162\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.101709119257059\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:01<00:00, 20.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 469.5053069330258\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 20.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 446.62044574945537\n==========================\nEpoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:15<00:00,  6.81it/s]\n100%|██████████| 5009/5009 [03:58<00:00, 21.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.135954491012101\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:10<00:00, 20.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.085518631935898\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:09<00:00, 20.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 462.1800302354338\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 439.44766517550084\n==========================\nEpoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:34<00:00,  6.64it/s]\n100%|██████████| 5009/5009 [04:02<00:00, 20.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.127915213369135\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 20.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.077179918638881\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [03:56<00:00, 21.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 458.47933205273574\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:07<00:00, 21.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 435.79847301541497\n==========================\nEpoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:11<00:00,  6.85it/s]\n100%|██████████| 5009/5009 [04:00<00:00, 20.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.123072539876984\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.072145927230588\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:05<00:00, 20.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 456.26443368805735\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:10<00:00, 20.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 433.6101797922894\n==========================\nEpoch: 7\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:22<00:00,  6.75it/s]\n100%|██████████| 5009/5009 [03:59<00:00, 20.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.119444999933063\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 20.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.068382822261607\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:00<00:00, 20.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 454.6123146055378\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 431.98152549060165\n==========================\nEpoch: 8\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:39<00:00,  6.60it/s]\n100%|██████████| 5009/5009 [04:08<00:00, 20.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.115317517471806\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:12<00:00, 19.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.064094317892639\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:02<00:00, 20.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 452.73977734108587\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:07<00:00, 21.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 430.1329375032078\n==========================\nEpoch: 9\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:08<00:00,  6.88it/s]\n100%|██████████| 5009/5009 [04:06<00:00, 20.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.109190127126965\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.057757713867216\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [04:04<00:00, 20.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 449.97414570591434\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:09<00:00, 20.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 427.4159726562338\n==========================\nEpoch: 10\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [12:16<00:00,  6.81it/s]\n100%|██████████| 5009/5009 [04:01<00:00, 20.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on train set: 6.100343896105659\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 21.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss on validation set: 6.048758329737336\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5009/5009 [03:56<00:00, 21.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on train set: 446.0111252009286\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1437/1437 [01:08<00:00, 21.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity on validation set: 423.58674830915595\n==========================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 717/717 [00:34<00:00, 21.08it/s]","output_type":"stream"},{"name":"stdout","text":"Test perplexity: 427.2778\n==========================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = LSTM_LanguageModel(vocab_size=len(train_ds.vocab))\n\ntrain_dl = DataLoader(train_ds, batch_size=128)\ndev_dl = DataLoader(dev_ds, batch_size=128)\ntest_dl = DataLoader(test_ds, batch_size=128)\n\ntrain_perplexities, val_perplexities = model.train_model(\n    num_epochs=10, \n    lr=0.1, \n    train_dl=train_dl, \n    dev_dl=dev_dl\n)\n\nmodel.get_perp(train_dl, filename='train_perplexity.txt')\nmodel.get_perp(test_dl, filename='test_perplexity.txt')\nprint(\"Perplexity files generated for both train and test sets.\")","metadata":{},"execution_count":null,"outputs":[]}]}